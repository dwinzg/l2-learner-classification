{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COLX 521 Lab Assignment 4: L2 Learner Classifier Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment you will \n",
    "- preprocess text derived from HTML documents\n",
    "- extract corpus linguistic features\n",
    "- Built a supervised classification system to identify language background "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### How does this relate to what we're doing in class\n",
    "\n",
    "This week, we're talking a lot about preprocessing and data storage.  We don't do these just because they're fun - although they are.  We need to do preprocessing and cleaning so that our data is more comprehensible to the algorithms that you've been working with for several months, now.  Language data is messy, it's ambiguous, it's noisy, but that just means it's a challenge that needs smart people to solve the puzzle.\n",
    "\n",
    "This lab serves as a combination of everything you've learned in COLX 521 - how to work with HTML, how to collect document stats (and now, use them as features!), how to find linguistic information through string and regex information, how to use lexicons for style and domain identification... Now, you get a chance to use these freely in a class competition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab you will be creating an end-to-end classification system using a corpus of HTML documents scraped from the old Lang-8 language learning website (No longer active). You will be building a classfier to solve a task called \"Native Language Identification\" (NLI).  In this task, we look at documents written by second-language learners, and try to identify the native language of the speakers.  The texts in the scraped corpus are English texts written by Lang-8 users whose native language (L1) is either another European language (French and Spanish) or L1 an East Asian languages (Japanese, Korean, and Mandarin Chinese). This is a group project where your team will submit a single solution, see details on submissions below.\n",
    "\n",
    "A bit of historical linguistics (Franz Bopp would be proud!): French and Spanish are closely related languages, being both part of the Romance branch of the Indo-European languages (a group of languages that spans from England (English, Gaelic, etc.) in the west to Iran (Farsi, Kurdish), to India (Hindi, Sanskrit) in the East, and Russia (Russian) in the north. Japanese, Korean and Mandarin are not considered related by linguists (although they share some characteristics through centuries of contact). Therefore, from a purely linguistic perspective, our task is kind of artificial (imagine classifying whether a fruit is either a citrus fruit, or a \"red\" fruit containing apples, cherries, and plums). However, there is a key distinction between European and East Asian languages in terms of their relationship to the \"L2\" (second language) in this case, English. \n",
    "\n",
    "Although English is in a different branch of the Indo-European language family (Germanic), Spanish and French nonetheless share numerous linguistic similarities with English (particularly with regards to having a shared educated vocabulary - we can mostly thank some guy named \"William the Conqueror\"), whereas the East Asian languages, having developed quite separately, have only coincidental similarities with English. One might presume this linguistic distance generally makes English more challenging for East Asian learners, who are in essence \"starting from scratch\" in a way that Spanish and French speakers are not (in fact, groups like the American Council of Teachers of Foreign Languages (ACTFL) and Interagency Language Roundtable (IRL) both consider east Asian languages to be among the most difficult for native English speakers to learn.  For more information, check out [this](https://www.mustgo.com/worldlanguages/language-difficulty/) website.). You may want to consider this as you brainstorm possible features! You might also reflect on general cultural differences between East Asia and Europe, and how they might be reflected in linguistic features you can easily extract. Or you can put aside these sorts of assumptions, and just let the data lead you! It is up to you.\n",
    "\n",
    "You will probably want to divide this project among members of your team according to their skills, schedule, and interest. Everyone must make a significant contribution. Although everything in the project is to some degree dependent on the rest, it is quite feasible to write most of the code for each part without having any of the other parts yet complete/working, provided you agree on the interface between the parts. For example, once you agree on the exact representation for each text, different team members can write functions which extract features for Part 2 even if the preprocessing code in Part 1 which creates that representation isn't yet ready. ***Talk to your group early***, and figure out how you are going to tackle it. **Any student which has failed to get in touch with their group before Thursday will lose 25% percent of the grade per day, starting on Thursday.**  If you haven't heard from a teammate, let Garrett know.\n",
    "\n",
    "Before you start serious work on this project, be sure to read **all** the instructions below, so you have a clear idea of the big picture in terms of what you are trying to do."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tidy Submission\n",
    "\n",
    "rubric={mechanics:2}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the marks for tidy submission:\n",
    "\n",
    "- One person in each group must create a private UBC github repo, and give access to all group members as well as Garrett (@gnicolai), Changbing (@cyang33), and Weirui (@wrchen)\n",
    "- In the README in the individual lab repo (the one created when the lab is opened) for all members of the group, you should have a link to this private, shared repo. Pushing that link is your only \"submission\". Don't put anything else in your repo for this lab.\n",
    "- In the README of the private shared repo, include instructions that will allow someone to reproduce your results and identify the parts of the code relevant to the major sections discussed below. \n",
    "- You do NOT have to use the ipynb you are reading; You may find it useful to have multiple ipynbs (or even standalone .py files which your .ipynb(s) import) for different parts of the assignment, just make sure you indicate how your code should be run (You should assume it will be run). You can also just add cells to this ipynb, whatever works for you.\n",
    "- Note that any commits on the private shared repo after the deadline will result in a late penalty being applied to the project, so be careful about that.\n",
    "- **Work should not be conducted on the main branch of the repo**.  Each teammate should have their own \"working branch\".  When you are ready to push code to the main branch, you should open a pull request.  Each pull request must be reviewed by another team member before being accepted to the main branch.  Teams should discuss amongst themselves how code review will be handled - it's fine to have teammates \"pair-up\", with members reviewing each other's code, or to have each team member assigned to another's pull requests.  The only requirement is that each team member must perform code review.  *Do not blindly accept pull requests to the main branch!*  This is how codebases get broken by bad code!  It often takes more time to find the bug introduced by the pushed code after it is in the main database than it does through a simple code review.  **If any members of the team are pushing blindly to the main branch, the entire team will receive a 20% penalty on the project.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Text extraction from HTML\n",
    "rubric = {accuracy:4,efficiency:1,quality:2}\n",
    "\n",
    "You first need to write code to extract relevant information from a large collection of HTML documents from the Lang-8 language learning website (lang-8.zip), available in the students repo [here](https://github.ubc.ca/MDS-CL-2025-26/COLX_521_corp-ling_students/tree/master/data/lang-8.zip). You should start by downloading this corpus and putting it in your shared repo. You may choose to extract the files from the zip manually, or you can pull the documents from the zip file within your code using the `zipfile` package (see discussion in Lecture 8). Both are okay, though the zipfile version is slightly prefered since having thousands of files on github is a bit messy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each html file consists of an online diary entry written by a second language learner of English. There are three things that your code will need to pass on to the next part of the process, for each file:\n",
    "\n",
    "- the native language of the writer (L1)\n",
    "- the raw text (string) of the entry, with HTML removed\n",
    "- the original filename (so that you can use our provided train/dev/test split) \n",
    "\n",
    "You'll need to inspect a sample file to identify where this information is contained in the HTML, and then write code to pull it out.\n",
    "\n",
    "Quality and efficiency tips:\n",
    "- Don't forget to document!\n",
    "- There are a couple of fairly independent tasks here, so you'll probably want multiple functions\n",
    "- However, what can be shared among functions, should be shared (i.e. don't have two functions load the same file twice!).\n",
    "- Ideally, your main function which iterates over all the documents should be a generator (with a `yield` statement) that allows Part 2 code to process the documents one by one, rather than having the entire corpus in memory at once. \n",
    "- If you choose to have multiple ipynbs, however, you can also choose instead to save the info you need from each file (not necessarily in separate files, you might keep it all in one file) and load it separately in the Part 2 ipynb. As long as you do this in a way that doesn't involve keeping the whole corpus in memory, you can still get full efficiency points.\n",
    "- You will want to make appropriate use of packages we have talked about in this course and their methods, which can save you a lot of work (e.g. BeautifulSoup's `get_text`)\n",
    "- Create a few tests for each function you write.  Good tests evaluate normal cases, but also \"edge cases\" - cases that are a little different from normal cases.\n",
    "- You can build this all into a class if you'd like, but that isn't necessary for full quality points, functions are fine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Feature Extraction\n",
    "rubric = {accuracy:6,efficiency:1,quality:2}\n",
    "\n",
    "The next step is to go from a general text representation to a set of features for each text, in the right format to be fed into a classifier. \n",
    "\n",
    "- You will first want to apply general text preprocessing techniques (Lecture 7) to get a useful Python data structure that isn't just raw text (it's also okay if you did this in the code included in Part 1!).\n",
    "- Then you should extract some features from the texts. You have significant leeway which features you extract, but for full points you need to follow the rules below:\n",
    "    - Your final system **can only have 15 features**. You may choose to test more than that using the dev set, but you will have to drop some before you do your final test. Do NOT use bag-of-word features here!  \n",
    "        - Any feature which has a categorical (string) value is actually a different feature for each categorical value (it becomes \"one-hot\".  ie, it is set to \"True\", and every other feature of that category is set to \"False\"). All your features for this project should probably have values that are either numerical or boolean, which result in a single feature.\n",
    "    - You must have at least one boolean feature based on a lexicon, meaning a feature which is True (or 1) if a word from a set of words you decide on is somewhere in the text, or False (0) otherwise. Some more rules for your lexicon(s):\n",
    "         - A lexicon must contain at least three words\n",
    "         - If it contains more than 10 words, you must store it in a separate text file and load it \n",
    "         - Although you may use purely statistical means (i.e. see \"comparing corpora\" in Lecture 4) to identify promising words, the lexicon must have an interpretable meaning other than a set of words that make the classifier do better. There needs to be something that ties all these lexical items together in a human interpretable way. E.g. {\"car\", \"truck\", \"train\", \"boat\"} are all modes of transport.\n",
    "         - This is different than BOW features - in BOW features, you have one feature for each word; this is a single feature that indicates if *any* of the words in the lexicon are in the document.  That is, if any word from the lexicon is in the document, this feature is True.  If not, this feature is False.\n",
    "    - You must have at least one \"statistic\", of the sort we saw in Lecture 4, one that is **not** just the count of a specific lexical item (or a lexicon of items) in a text.\n",
    "    - You must have one feature that involves doing sentence segmentation.\n",
    "    - You must have one feature that involves doing part-of-speech tagging.\n",
    "    - You must have one feature that involves doing lemmatization or stemming. \n",
    "    - A single feature can satisfy more than one of the requirements above. For example, you might have a statistic that relies on sentence segmentation, satisfying both the statistic and sentence segmentation requirement.\n",
    "    - You can do preprocessing as part of your feature extraction. For example, POS tagging and lemmatization are actually somewhat incompatible, and so you may want to do those independently\n",
    "    - If you look at the corpus to develop your features (a good idea) you should make sure that you look in the training set, not the dev or test sets.  It is bad science to extract features from your dev or test set.  The whole point of these sets is to see if a model learned on training data can be generalized to unseen data.  If we extract features from either of these sets, we are likely to get results that are unfairly inflated - if we apply the model to other data, it won't be as successful.\n",
    "\n",
    "- As you are building these feature dictionaries (your \"X\"), you will be also be creating the list of classifications (your \"y\"). If a text has French or Spanish as the L1, it should belong to the \"European\" class; likewise, if the text has Mandarin, Japanese, or Korean as the L1, it belongs to the \"Asian\" class. There are also Russian texts in the corpus, but we are not going to use them.\n",
    "- Before you create the feature dictionaries, you will want to load the list of filenames found in `train.txt`, `dev.txt`, and `test.txt` files (in the same directory as `lang8.zip`, [here](https://github.ubc.ca/MDS-CL-2025-26/COLX_521_corp-ling_students/tree/master/data)) and use them to create 3 separate lists of feature dicts (and corresponding lists of classifications, i.e. \"European\" and \"Asian\") for each of the train/dev/test splits, using the filename returned from Part 1. Note that the Russian texts we are not using are included in the splits, so you should not create the splits by iterating over train/dev/text.txt, but rather iterate over all the documents in the corpus and put the extracted features for relevant texts in the appropriate list, based on whether they are in the lists of train, dev, or test sets you have already loaded beforehand.\n",
    "  \n",
    " Quality and efficiency tips:\n",
    " \n",
    " - Again, don't forget to document!\n",
    " - You will probably want to have an independent function for extracting each particular feature from one text, unless...\n",
    " - If you have multiple features that can be generalized into a single function, that can be good too!\n",
    " - Again, make sure things that can be shared are shared (e.g. POS tagging), don't redo the same processing for multiple features\n",
    " - Make your feature names easily interpretable, even if they're a bit long (the ML tool doesn't care if the feature is called X or number_of_times_the_appears_in_text, but the second one is much easier to understand when debugging and maintaining code)\n",
    " - In addition to the individual functions for each feature, you'll probably want to have a single function that builds a feature dictionary for one text by calling those functions.\n",
    " - You should have individual tests that show each feature extraction function is working correctly!  Good tests make code review easier, too!\n",
    " - Make sure you convert the filenames listed in `train.txt`, `dev.txt`, and `test.txt` into sets, so that looking up which the correct \"split\" for each lang-8 entry is fast."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Classification\n",
    "rubric = {accuracy:4,efficiency:1,quality:1}\n",
    "\n",
    "Now you're going to build and test a machine learning model based on your features\n",
    "\n",
    "- First, vectorize your lists of feature dictionaries appropriately, turning them first into sparse matrices, and then numpy arrays (using `toarray`). With so few features, you don't have to worry about keeping things sparse, and having dense arrays will be useful later.\n",
    "- Build a sklearn DecisionTreeClassifier with max_depth=3 (for regularization), and get a preliminary [accuracy_score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html) on the _dev_ set using all features. You may choose to modify the classifier and hyperparameters settings to get better results, see discussion in \"Part 5\" below.\n",
    "- Next, you're going to do feature [ablation](https://en.wikipedia.org/wiki/Ablation_(artificial_intelligence%29), which involves removing features one at a time from your model, and seeing what the effect is. Report results with each feature missing (make it clear which feature is missing for each result you present). If the results go down when you remove the feature, it means the feature is probably useful, and if the results go up or stay the same, then it is probably not.  For example, if your accuracy is 70% with all features, and you remove some feature X, and the accuracy drops to 68%, then that feature is at least partially responsible for an additional 2% absolute improvement (often measured as a relative increase, called \"error reduction\" - in this case (32-30)/32 = 6.25% error reduction) \n",
    "- The last step of the coding part of the lab is checking accuracy of your model on the _test_ set, after removing any features that are not useful based on _dev_ set results. However, only run this step when you are completely finished with this assignment (you should not be updating your model based on test set results!).  In other words, find the best accuracy you can on the development set, and then freeze the model so you are doing no more training.  Apply the best model to the test set (1 time only), and report those results. Before you do that, please modify and include the assert below which confirms that your final dataset has only 15 features (change X to whatever you are calling your training matrix).  We use the development set to find the best features / accuracies, and then use the learned model on the test set - this mimics a real-world setting where you likely have never seen the final data you will be running your model on.  The train/dev/test split gives us a better idea of how our models will work on real data.\n",
    "\n",
    "Quality and efficiency tips:\n",
    "\n",
    "- The most efficient way to do feature ablation is simply to remove a column (check out for instance [np.delete](https://numpy.org/doc/stable/reference/generated/numpy.delete.html). Note that delete doesn't work right with sparse matricies, so you'll definitely want to turn your sparse matrices into ndarrays before doing this. For each feature, you will have to remove it from both the training and dev matrices before building and testing the model.\n",
    "- There are a few natural ways to modularize into functions, and avoid repeated code. For instance, you can have a general function for deleting a feature from any feature matrix, and a function which does the entire ablation process for one provided feature: given feature matrices and one feature, the function would delete the feature from the matrices, train the model on the ablated training matrix, and test the model on the ablated test matrix.\n",
    "- As always document your functions, and add a few tests to show how they work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Report\n",
    "rubric={raw:2,reasoning:4,viz:1,writing:2}\n",
    "\n",
    "You need to write a short report about your project, namely:\n",
    "\n",
    "- What features did you use, and why did you use them? You need to be able to say something more about each feature than just \"to satisfy the lab requirements\". You can discuss how they relate to your assumptions about differences, what you saw in the texts (if you looked at some), or any statistical analyses you did to help identify features. You will lose points here if there is no rational basis for your selection of features.\n",
    "- How did the features perform? Did they work as you expect? What did you end up dropping? To deepen your discussion, you might want to display the decision tree using [sklearn.tree.plot_tree](https://scikit-learn.org/stable/modules/generated/sklearn.tree.plot_tree.html).And you should include a simple graph or table showing your results.\n",
    "- Please discuss what each member of your group did. Though it is not necessary that every group member has an equal contribution in terms of code, every group member should have a significant contribution, a major part of the lab for which they were a primary contributor. If any team member fails to contribute significantly, the rest of the group, assuming they have made efforts to encourage the team member to contribute, should discuss what happened in the report, otherwise all team members might lose the raw points for this rubric. Team members who failed to contribute may lose some or all of their grade for the assignment.  Make sure to also outline how code review was performed.  This should be similar to your weekly teamwork reports.\n",
    "- Perform a short qualitative analysis of the data - If your classifier is making mistakes, investigate a couple of the errors, and create a hypothesis for why that might be.  It might be that the examples look like L1 English (and thus are difficult to determine the L1), or it might be that they heavily influence one of your features incorrectly.  You should provide 3 misclassified examples from each class.  Your hypothesis can address each example individually, or it can generalize over multiple examples. \n",
    "- I always get asked \"how long should the report be?\", and I always answer \"as long as it needs to be\".  To discuss all of the above items, you'll probably need a few pages, but if you want to include images (which require more space), they are always welcome.  Don't just drop images into your text, though - if you have an image, there should be a reason, and you should discuss the image in the text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Optional competition\n",
    "rubric={raw:2}\n",
    "\n",
    "If you like, you may continue to develop your model, including features or better algorithms/hyperparameters, and test your skills against the other teams in the class.\n",
    "\n",
    "Your team score is calculated via two metrics - your accuracy, and the number of features you use; make sure your features are efficient!\n",
    "\n",
    "Score = Accuracy - # of features + 0.5 * number of unique features.  Accuracy is our prime focus, but if you can achieve high accuracy while limiting the number of features, and the features are not just the same old things...\n",
    "\n",
    "So if your best classifier gets 75% accuracy, and uses 5 features, 3 of which *nobody else in the course used*, then you will get a score of 71.5 (75 - 5 + (3 * 0.5)).\n",
    "\n",
    "A feature is unique not if it just picks different words to represent - it needs to be a different \"class\" of features.  Be clever.  Experiment.  The reason that I don't reward uniqueness at full points is because it's quite easy to overload your model with useless features that cancel out the \"too many features\" penalty.\n",
    "\n",
    "The teams with the top 4 scores in the course on the _test_ set will get bonus points as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1st place: 2 bonus <br>\n",
    "2nd place: 1.5 bonus <br>\n",
    "3rd place: 1 bonus <br>\n",
    "4th place: 0.5 bonus <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the case of a tie, the relevant points are split between teams. All the rules for features apply for this competition, as well, you can't have more than 15 features in your final model, so you need to make them good! For your final submission, you should use the model with your best score on the dev set. Any sign that you have developed your model using the test set will invalidate the final score you get and make it impossible to get any bonus points.\n",
    "\n",
    "Good luck!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:block1_env]",
   "language": "python",
   "name": "conda-env-block1_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
